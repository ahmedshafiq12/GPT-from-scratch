{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mounting Drive and Change Directory**"
      ],
      "metadata": {
        "id": "ycd9fYN7_1rn"
      },
      "id": "ycd9fYN7_1rn"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mc7hNYzRIm2",
        "outputId": "16628b55-6ad3-4825-b407-081b5a9a07f9"
      },
      "id": "-mc7hNYzRIm2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/AI Projects/GPT-from-scratch\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-a5EweGRTN0",
        "outputId": "4d0c2ffc-c061-4fcd-be0e-a8914cb6c41a"
      },
      "id": "7-a5EweGRTN0",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI Projects/GPT-from-scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "JmSMn7papdTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67465a9a-9e2e-4d49-8cee-a1b0d9d9ac44"
      },
      "id": "JmSMn7papdTC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_extract.py  dataset  GPT  README.md  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74q1TaGHRWRq",
        "outputId": "88218a27-afe9-4efd-d9d6-644a23317d05"
      },
      "id": "74q1TaGHRWRq",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 1))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: filelock==3.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.14.0)\n",
            "Collecting fsspec==2024.5.0 (from -r requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting intel-openmp==2021.4.0 (from -r requirements.txt (line 4))\n",
            "  Downloading intel_openmp-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.1.5)\n",
            "Collecting mkl==2021.4.0 (from -r requirements.txt (line 7))\n",
            "  Downloading mkl-2021.4.0-py2.py3-none-manylinux1_x86_64.whl (280.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.9/280.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.3.0)\n",
            "Collecting networkx==3.2.1 (from -r requirements.txt (line 9))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (24.0)\n",
            "Requirement already satisfied: sympy==1.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.12.1)\n",
            "Requirement already satisfied: tbb==2021.12.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2021.12.0)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm==4.66.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.66.4)\n",
            "Requirement already satisfied: typing_extensions==4.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (4.12.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 13)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: intel-openmp, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, mkl, fsspec, colorama, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: intel-openmp\n",
            "    Found existing installation: intel-openmp 2023.2.4\n",
            "    Uninstalling intel-openmp-2023.2.4:\n",
            "      Successfully uninstalled intel-openmp-2023.2.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: mkl\n",
            "    Found existing installation: mkl 2023.2.0\n",
            "    Uninstalling mkl-2023.2.0:\n",
            "      Successfully uninstalled mkl-2023.2.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 fsspec-2024.5.0 intel-openmp-2021.4.0 mkl-2021.4.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Data**"
      ],
      "metadata": {
        "id": "91UboJ4DAJsG"
      },
      "id": "91UboJ4DAJsG"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/AI Projects/openwebtext/openwebtext.zip\" -d \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqRA7V6jmTny",
        "outputId": "6fb16804-4995-4702-eaa1-26ad3477a52c"
      },
      "id": "hqRA7V6jmTny",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/AI Projects/openwebtext/openwebtext.zip\n",
            "   creating: /content/openwebtext/\n",
            "  inflating: /content/openwebtext/urlsf_subset08.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset06.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset15.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset20.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset07.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset01.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset10.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset13.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset03.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset19.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset09.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset00.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset16.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset11.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset02.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset14.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset04.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset12.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset18.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset17.tar  \n",
            "  inflating: /content/openwebtext/urlsf_subset05.tar  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "def extract_all_tar_files(input_dir, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # List all tar files in the input directory\n",
        "    tar_files = [f for f in os.listdir(input_dir) if f.endswith('.tar')]\n",
        "\n",
        "    for tar_file in tar_files:\n",
        "        tar_path = os.path.join(input_dir, tar_file)\n",
        "        with tarfile.open(tar_path, 'r') as tar:\n",
        "            tar.extractall(path=output_dir)\n",
        "            print(f'Extracted {tar_file} to {output_dir}')"
      ],
      "metadata": {
        "id": "Yu4aXDccReXb"
      },
      "id": "Yu4aXDccReXb",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = '/content/openwebtext'  # Change this to your directory containing tar files\n",
        "output_dir = '/content/extracted_openwebtext'  # Change this to your desired output directory\n",
        "\n",
        "# Extract all tar files\n",
        "extract_all_tar_files(input_dir, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2stq1uD7oHZZ",
        "outputId": "0896126e-ece1-4624-deb2-611352133e0e"
      },
      "id": "2stq1uD7oHZZ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted urlsf_subset12.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset09.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset08.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset01.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset10.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset19.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset07.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset15.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset06.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset04.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset20.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset13.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset14.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset18.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset02.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset16.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset11.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset03.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset05.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset00.tar to /content/extracted_openwebtext\n",
            "Extracted urlsf_subset17.tar to /content/extracted_openwebtext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_extract.py --folder_path \"/content/extracted_openwebtext/openwebtext\" --output_folder \"/content/dataset\" --sample_rate 0.06"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYD9pYhdo_UF",
        "outputId": "814bfc1e-adfd-4fed-d650-da97831ebc82"
      },
      "id": "OYD9pYhdo_UF",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 1112/1112 [01:32<00:00, 12.05it/s]\n",
            "100% 123/123 [00:10<00:00, 12.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "mJRz95vXAC2J"
      },
      "id": "mJRz95vXAC2J"
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset --hard HEAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nabta6HOCXYl",
        "outputId": "b3a084da-0871-43a6-e303-98ff5dbdfd74"
      },
      "id": "nabta6HOCXYl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI Projects/GPT-from-scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yPtw710tEFt",
        "outputId": "f11a9694-27aa-4178-e890-80b91cf15fb4"
      },
      "id": "7yPtw710tEFt",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GPT.LanguageModel import GPTLanguageModel"
      ],
      "metadata": {
        "id": "lgjTyijMr0Vl"
      },
      "id": "lgjTyijMr0Vl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c351b7b9",
      "metadata": {
        "id": "c351b7b9"
      },
      "outputs": [],
      "source": [
        "# Configuration parameters (example values)\n",
        "n_embd = 768\n",
        "block_size = 128\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "dropout = 0.1\n",
        "learning_rate = 5e-4\n",
        "max_iters = 5000\n",
        "eval_iters = 200\n",
        "batch_size = 64\n",
        "\n",
        "# File paths\n",
        "dataset_path = \"/content/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7b3838bc",
      "metadata": {
        "id": "7b3838bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a47b6e-4bb9-4d3c-d87b-28cf54934fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Welcome!! I'm your GPT, developed by Ahmed Shafiq. 🚀\n",
            "🚀 I'm using cuda as a device\n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel(n_embd, n_layer, n_head, dropout, block_size, batch_size, dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model(max_iters, eval_iters, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn8lbzoDuRWl",
        "outputId": "deac7f1a-dd56-4e0e-ad78-0d5e8448306e"
      },
      "id": "tn8lbzoDuRWl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 9.374, val loss: 9.378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 97/5000 [06:26<2:12:12,  1.62s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d4668c",
      "metadata": {
        "id": "b0d4668c"
      },
      "outputs": [],
      "source": [
        "prompt = 'Hello! Can you see me?'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "print(generated_chars)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}